# Qdrant Configuration
qdrant:
  url: "QDRANT_URL"
  collection_name: "code_dna"

# Embedding Configuration
embeddings:
  # Provider: fastembed (local), openai (API), or huggingface (local)
  provider: "fastembed"

  # Model selection based on your needs:
  # Code-optimized (recommended):
  #   - jinaai/jina-embeddings-v2-base-code (768 dim, best for code)
  #   - BAAI/bge-base-en-v1.5 (768 dim, good all-around)
  # Lightweight:
  #   - BAAI/bge-small-en-v1.5 (384 dim, fast, current default)
  #   - sentence-transformers/all-MiniLM-L6-v2 (384 dim)
  # Large/High quality:
  #   - nomic-ai/nomic-embed-text-v1.5 (768 dim, newest)
  model: "jinaai/jina-embeddings-v2-base-code"

  # Vector dimensions (auto-detected, but can override)
  # vector_size: 768

  # Chunking strategy for large code files
  chunking:
    enabled: true
    max_chunk_size: 512  # tokens
    chunk_overlap: 50    # tokens overlap between chunks
    strategy: "smart"    # smart (respects code structure) or simple

  # Code-specific preprocessing
  preprocessing:
    normalize_whitespace: true
    include_comments: true
    include_docstrings: true
    remove_empty_lines: false

# Search Configuration
search:
  # Hybrid search combines semantic (vector) and keyword (text) matching
  hybrid_enabled: true
  semantic_weight: 0.7  # Weight for semantic similarity (0-1)
  keyword_weight: 0.3   # Weight for keyword matching (0-1)

# GitHub Configuration
github:
  # Token loaded from environment variable for security
  token_env_var: "GITHUB_TOKEN"

  # Repository filters
  include_private: true
  include_orgs: true

  # Repos to always exclude (glob patterns)
  excluded_repos:
    - "dotfiles"
    - "archived-*"
    - "*.github.io"

  # API Response Caching
  cache:
    enabled: true
    # TTL (time-to-live) in seconds for different cache types
    ttl_repo_list: 300      # 5 minutes for repository listings
    ttl_file_tree: 600      # 10 minutes for file tree structure
    ttl_file_content: 3600  # 1 hour for file content (by SHA, effectively immutable)
    # Maximum cache entries (LRU eviction when exceeded)
    max_size: 1000
    # Persistent cache directory (set to null to disable disk caching)
    cache_dir: ".github_cache"

# LLM Configuration
llm:
  # Provider: gemini, openai, or mock (for testing)
  provider: "gemini"

  # Model to use
  model: "gemini-2.0-flash"

  # Minimum quality score for patterns (1-10)
  min_quality_score: 3

  # Retry settings for rate limiting
  max_retries: 5
  initial_retry_delay: 1.0   # seconds
  max_retry_delay: 60.0      # seconds (cap for exponential backoff)

# Discovery Configuration
discovery:
  ignored_dirs:
    - ".git"
    - "node_modules"
    - "venv"
    - "__pycache__"
    - "dist"
    - "build"
    - "target"
    - ".idea"
    - ".vscode"
    - "vendor"
    - ".gradle"
    - "bin"
    - "obj"
  
  supported_extensions:
    - ".py"
    - ".ts"
    - ".js"
    - ".tsx"
    - ".jsx"
    - ".go"
    - ".java"

# Scaffolding Configuration
scaffolding:
  # Default output directory for new projects
  output_dir: "./generated_projects"

# Batch Processing Configuration
batch:
  # Files to process per batch (lower = less memory, more API calls)
  batch_size: 10

  # Rate limiting between batches (seconds)
  delay_between_batches: 0.5

  # Retry settings for transient failures
  max_retries: 3
  retry_delay: 1.0

  # Progress persistence for resumable syncs
  save_progress: true
  progress_dir: ".batch_progress"

  # Progress tracking intervals
  progress_save_interval: 10    # Save progress every N files
  progress_notify_interval: 50  # Notify callback every N patterns

# Repository Processing
repository:
  # Threshold for automatic batch processing (files)
  large_repo_threshold: 50

# Statistics Configuration
stats:
  # Batch size for scrolling through collection
  scroll_batch_size: 500

# Python Configuration
python:
  # Use system Python by default
  interpreter: "python"
